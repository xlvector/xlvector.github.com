---
layout: post
title: 基于Golang的分布式爬虫框架
categories: golang crawler
tags: golang crawler architecture
---

爬虫是数据搜集系统中的一个重要工具，主要用于从Web上搜集数据。一般一个爬虫的流程如下：

	func Crawler(seed_url):
		Q = Queue()
		Q.append(seed_url)
		while !Q.empty() :
			link = Q.pop()
			html = download(link)
			sub_links = extract_links(html)
			for link in sub_links:
				Q.append(link)

如果我们要设计一个分布式爬虫，可以将上述程序分成3个部分：

1. downloader : 输入一个url，返回这个url对应的HTML
2. link extractor : 输入一个HTML，返回这个HTML中的链接
3. redirector : 负责接收link extractor提取的链接，并且将这些链接转发给downloader

不过，因为downloader主要消耗网络资源，而link extractor主要消耗CPU资源，因此我们可以将1，2部分合并在一个程序中。我们把合并后的程序也称为downloader。downloader集群通过nginx做负载均衡和外面通信。而downloader集群的鲁棒性可以通过nginx的health check实现。

