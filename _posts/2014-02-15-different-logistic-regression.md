---
layout: post
title: Logistic Regression的几个变种
categories: hector
tags: hector machine_learning lr
---

最近几年广告系统成为很多公司的重要系统之一，定向广告技术是广告系统中的重要技术，点击率预估是定向广告技术中的重要组成部分，Logistic Regression是解决点击率预估最常用的机器学习算法。所以本文介绍一下Logistic Regression（下文简称LR）。

# 解决的问题

LR主要用来解决两类分类问题。下面的问题是一些典型的两类分类问题：

1. 用户看到一个广告后会点还是不会点
2. 一个人是男还是女
3. 一张图片中的图像是不是人脸
4. 一个人借钱后会不会还

两类分类问题是机器学习的基本问题，所有的分类算法至少都可以解决两类分类问题， 比如：

1. 决策树，随机森林，GBDT
2. SVM， Vector Machine
3. Gauss Process
4. 神经网络

那为什么点击率预估问题选择LR呢，主要是因为：

1. 数据规模很大，而LR无论是训练还是预测的计算复杂度很低
2. 特征很多，对特征做了线性变换，因此问题基本是线性的，线性分类器就可以解决
3. LR不仅可以预测一个样本属于那一类，而且可以给出属于每一类的概率
4. LR的模型简单，从而解释预测结果也相对容易
5. LR的模型简单，从而并行化相对容易

# 不同类型的LR

自从LR提出之后，学术界对它的改进主要基于两个方面：

1. 用什么样的正则化，早期是L2正则化，而最近用的比较多的是L1正则化
2. 用什么样的优化算法，如何在最短的时间内收敛到最优的解

## 正则化

正则化是机器学习中的一个重要技术，它的主要目的是让防止一个模型过拟合。目前比较常用的正则化有L1，和L2：

1. L2正则化认为特征的权重的先验分布是一个0附近的高斯分布
2. L1正则化认为特征的权重的先验分布是一个0附近的拉普拉斯分布

L1正则化相对与L2正则化有一个优点，就是加入L1正则化的损失函数在优化后，绝大多数特征的权重都是0。这个特性可以大大减少在线预估时的内存占用，并提高预测的速度，这是因为

* 在线预估的主要计算样本的特征向量x和模型的特征权重向量w的点乘
* w向量一般需要用HashMap存储，而一个特征的权重为0，就不需要存储了，因为HashMap中不存在的特征就是权重为0
* 所以L1正则化可以减少w的内存占用，而w减小后，计算w和x的点乘的速度也能提高

## 优化方法

L2正则化的LR的损失函数是一个可以求导的凸函数，从而可以用最速下降法（梯度法）进行优化。一般梯度法有3种

1. Batch
2. Mini batch
3. SGD （随机梯度法）

这3种方法是最早提出的优化方法。可以用梯度法，自然也可以用牛顿法来获得超线性收敛的特性，于是共轭梯度法和LBFGS也被用来优化LR。LBFGS是基于L2正则化的，如果基于L1正则化，微软提出了OWLQN算法（http://blog.csdn.net/qm1004/article/details/18083637）。

无论是梯度法还是拟牛顿法，它们都是频率学派的优化双方。它们其实是极大似然估计用了不同的优化算法。于是，贝叶斯学派也提出了Bayesian的优化算法

* Ad Predictor : 这是微软的研究员提出的一种算法， 论文可以参考 Web-Scale Bayesian Click-Through Rate Prediction for Sponsored Search Advertising in Microsoft’s Bing Search Engine。

Ad Predictor有几个比较好的特性

1. 它只需要扫瞄一次数据集就可以收敛到最优解，而不是像梯度法或者拟牛顿法那样需要反复迭代数据集。
2. 它不仅仅能预测出一个样本是正样本的概率，而且还可以给出对于这个概率预测值的置信度

Ad Predictor很好了，不过它是基于L2正则化的，这样总是让人不能满意。Google在2013年发表了一篇论文（Ad Click Prediction: a View from the Trenches），介绍了一个基于L1正则化的LR优化算法FTRL-Proximal，且又具有上述Ad Predictor的两个优点。

## 并行化

算法的并行化有两种

1. 无损的并行化：算法天然可以并行，并行只是提高了计算的速度和解决问题的规模，但和正常执行的结果是一样的。
2. 有损的并行化：算法本身不是天然并行的，需要对算法做一些近似来实现并行化，这样并行化之后的双方和正常执行的结果并不一致，但是相似的。

在前面提到的算法中，基于Batch的算法(Batch-GD, LBFGS, OWLQN)都是可以进行无损的并行化的。而基于SGD的算法（Ad Predictor， FTRL－Proximal）都只能进行有损的并行化。
